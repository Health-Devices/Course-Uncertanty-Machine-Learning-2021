{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local and global sensitivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ Definition of Local and Global Sensitivity Analysis\n",
    "+ ANOVA(analysis of variance) Decomposition\n",
    "+ Sobol indices\n",
    "+ Morris sensitivity analysis \n",
    "+ Sensitivity analysis in metrology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols\n",
    "D output variance \\\n",
    "$S_i$ sensitivity indices \\\n",
    "$S_{Ti}$ total sensitivity indices \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "### Local Sensitivity Analysis\n",
    "Local sensitivity analysis focuses on how the small perturbation near an input $x_i$ will influence Y output of the model. A common approach is to change only one factor each time, and detect the influence of this factor on the output, and repeat this process until all the factors are changed. This method is also called OAT(One factor At Time)method.\n",
    "### Global Sensitivity Analysis\n",
    "Global sensitivity focuses on how changes of multiple inputs affect the output of the model, and it enables to detect how the interaction between multiple inputs affect the output.\n",
    "### The goal of Global Sensitivity Analysis\n",
    "+ If we can rank how all the model factors contribute to the output of the model, we can better understand the model outputs and  find out the most effective way to improve our model.\n",
    "+ It also enables to partition output variance by detecting the factors of the model, so that we can find the main value,main effects  and all the interactions.\n",
    "<img src=\"goal.png\",width = 350,height = 350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA(analysis of variance) Decomposition\n",
    "+ High-Dimensional Model Representation of f(x) in case $x$ follow uniform distribution U(0,1):  [4]\n",
    " $f(x)=f_0+\\sum_{i}f_i(x_i)+\\sum_{i<j}f_{(ij)}(x_i,x_j)+...+f_{12...n}(x_1,x_2,...,x_n)$\n",
    " \n",
    " the model is partition into several parts, $f_0$ is the mean value,$\\sum_{i}f_i(x_i)$ represents the sum of the single variable $x_i$,and is also called main effects, $\\sum_{i<j}f_{(ij)}$ is the sum of bases which involve two variable interaction,this part is called First-order interactions,and so on.\n",
    "   \n",
    " This is an example of high dimensional model representation HDMR for a function of three parameters\n",
    "$f(x)=f_0+f_1(x_1)+f_2(x_2)+f_3(x_3)+f_{(12)}(x_1,x_2)+f_{(13)}(x_1,x_3)+f_{(23)}(x_2,x_3)+f_{(123)}(x_1,x_2,x_3)$\n",
    "\n",
    " we can see in this function,$f_0$ is the mean value,and $f_1(x_1)+f_2(x_2)+f_3(x_3)$ is the main effects part,if want to improve this model we can foucs on this part,and then $f_{(12)}(x_1,x_2)+f_{(13)}(x_1,x_3)+f_{(23)}(x_2,x_3)$ is the First-order interactions,and $f_{(123)}(x_1,x_2,x_3)$ is the Second-order interaction.\n",
    "+ Calculate the ANOVA decompostion\n",
    "  + the main value of the model is to calculate the expectation of the model\n",
    "$$f_0=E[f]$$\n",
    "  + the main effect part is calculated by expectation of the output of single variable minus the main value\n",
    " $$f_i=E[f\\mid x_i]-f_0$$\n",
    "  + the two variables interaction is calculated by expectation of the output of two varibales minus the main effect parts and main value part\n",
    " $$f_{i,j}=E[f\\mid x_i,x_j]-\\sum_{i} f_i-f_0$$\n",
    "  + from the previous fuction we can get the funtion of mutiple variables interactions\n",
    " $$f_{1,2,3,...,m}=E[f(x)]-'everything else'$$\n",
    "\n",
    "Here is a example for calculating the ANOVA decompostion \n",
    "\n",
    "Given a model:$y=ax_1+bx_2^2+cx_1x_2+d$\n",
    "\n",
    " $f_0=\\int_0^1\\int_0^1 ydx_1dx_2=\\int_0^1(\\frac{a}{2}+bx_2^2+\\frac{c}{2}x_2+d) dx_2=\\frac{a}{2}+\\frac{b}{3}+\\frac{c}{4}+d$\n",
    "\n",
    " $f_1(x_1)=\\int_0^1ydx_2-f_0=ax_1+\\frac{b}{3}+\\frac{c}{2}x_1+d-f_0=a(x_1-\\frac{1}{2})+c(\\frac{x_1}{2}-\\frac{1}{4})$\n",
    "\n",
    " $f_2(x_2)=\\int_0^1ydx_1-f_0=\\frac{a}{2}+bx_2^2+\\frac{c}{2}x_2+d-f_0=b(x_2^2-\\frac{1}{3})+c(\\frac{x_2}{2}-\\frac{1}{4})$\n",
    "\n",
    " $f_{12}(x_1,x_2)=y-f_1-f_2-f_0=c(x_1x_2-\\frac{x_1}{2}-\\frac{x_2}{2}+\\frac{1}{4})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobal indices\n",
    "Variance-based sensitivity analysis (often referred to as the Sobol method or Sobol indices, after Ilya M. Sobol) is a form of global sensitivity analysis[1].\n",
    "### Calculating Variances\n",
    "to calculate the Sobol indices,we should first calculate the Varances.\n",
    "+ Given a function f(x) that is square integrable, \n",
    "  Variance of f can be represented as:\n",
    "  $$V=\\int{f(x)^2dx-f_0^2}=D$$\n",
    "+ Similarly we can get Variance of $f_i$,\n",
    "  $$V_i=\\int_0^1{f_i(x_i)^2dx_i}=D_i$$\n",
    "+ we can also get the Variance of $f_{ij}$:\n",
    "  $$Var_{ij}=Var[E[f\\mid x_i,x_j]]-V_i-V_j=D_{ij}$$\n",
    "+ so the Variance of model Y:\n",
    "  $$Var(Y)=\\sum_{i=1}^{d}V_i+\\sum_{i<j}^{d}V_{ij}+...+V_{12...d}$$\n",
    "### Monte Carlo Estimates\n",
    "in the Variance calculating part,we need the expectation of f(x),we use Monte Carlo method to get E[f(x)]\n",
    "+ The expected value of f(x) may be estimated from:\n",
    "$$\\hat{f_0}=\\frac{1}{N} \\sum_{m=1}^{N} f(\\bar{x}_m)$$\n",
    "+ The variance of f(x) may then be estimated from:\n",
    "$$\\hat{D}=\\frac{1}{N} \\sum_{m=1}^{N} f(\\bar{x}_m)^2-\\hat{f_0}^2$$\n",
    "+ For the one indexed terms the variance may be estimated from:\n",
    "$$\\hat{D_i}=\\frac{1}{N} \\sum_{m=1}^{N} f(\\bar{u}_m,x_i)f(\\bar{v}_m,x_i)-\\hat{f_0}^2$$\n",
    "### First-order Sensitivity  indices\n",
    "the First-order Sensitivity indices is a direct variance-based measure of sensitivity Si can be calculated by:\n",
    "$$S_i=\\frac{V_i}{Var(Y)}$$\n",
    "+ The main (or first-order) effect (Si) measures the contribution to the output variance from varying the i-the factor alone (but averaged over variations in other factors)\n",
    "\n",
    "(i) the higher the value of Si, the higher the influence of the i-th factor on the output\n",
    "\n",
    "(i) if Si = 0, then the i-th parameter has no direct influence on the output (but it might still have some in interaction with other parameters!)\n",
    "\n",
    "(iii) the sum of all Si is always lower or equal to 1. If it is equal to 1, then there are no interactions between the parameters (“additive” model).\n",
    "### Total Sensitivity Indices\n",
    "+ Sensitivity indices:\n",
    "$$\\hat S_i=\\frac{\\hat{D_i}}{\\hat{D}},\\sum_{S} \\hat{S_k}=1$$\n",
    "+ Total Sensitivity indices:\n",
    "$$\\hat S_{Ti}=1-\\hat S_{i^c}=1-\\frac{\\hat{D_{i^c}}}{\\hat{D}}, \\sum_{i} \\hat{S_{Ti}}\\geq1$$\n",
    "+ The total effect (STi)\tmeasures the total contribution to the output variance of the i-th factor, including its direct effect and interactions with other factors\n",
    "\n",
    "(i)  STi must be higher or equal to Si. If it is equal, then the parameter has no interactions with the other parameters\n",
    "\n",
    "(ii)  if STi = 0, the i-th parameter has no influence (neither direct or indirect) on the model output\n",
    "\n",
    "(ii) the sum of all STi is always higher or equal to 1. If it is equal to 1, then there are no interactions between the parameters\n",
    " ### Sensitivity indices - Advantages and Disadvantages\n",
    "+ Advantages\n",
    "Extremely robust, they work with any type of discontinuous (even randomised) mapping between input factors and the output. Sobol’ estimator is unbiased. They do not rely on any hypothesis about the smoothness of the mapping. The only key assumption is that variance (i.e. the second moment) is an adequate measure for quantifying the uncertainty of the model output.\n",
    "Computing main effects and total effects for each factor, while still being far from a full factors mapping, gives a fairly instructive description of the system. Moreover, they provide unambiguous and clear answers to well specified sensitivity settings (prioritisation and fixing).\n",
    "+ Disadvantages\n",
    "The computational cost is relatively high, which implies that these methods cannot be applied to computationally expensive models. They do not provide any mapping, i.e. they decompose the output uncertainty but they do not provide information about, e.g., the input factors responsible for producing Y values in specified regions, such as extreme high/low or any behavioural classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "(This problem is from Chapter 4 of the book: Andrea Saltelli, Debora Gatelli, Francesca Campolongo, Jessica Cariboni, Marco Ratto, Michaela Saisana, and Terry Andres, Global Sensitivity Analysis: The Primer, Wiley, 2008.)\n",
    "\n",
    "Two input factors are normally distributed in the model \n",
    " $Y = X_{1}+ X_{2}$\n",
    "with parameters $\\mu_{1}=1$, $\\mu_{2}=2$, $\\sigma_{1}=3$ , $\\sigma_{2}=2$.\n",
    "\n",
    "a) Calculate the first and second-order indices for the inputs and comment on the level of additivity of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $X_1$ and $X_2$ are normal random variable and there is no relation between them, or there is no reason for any dependency, then: $$E(Y) = E(X_1) + E(X_2) \\implies \\\\ E(Y) = 1 + 2 = 3$$ \n",
    "\n",
    "Likewise: $$var(Y) = var(X_1) + var(X_2) = 4 + 9 = 13$$\n",
    "\n",
    "To calculate the first order index we need first to calculate the $f_i(x)$ which correspond to $f_1(x)$ and $f_2(x)$.\n",
    "\n",
    "$$f_1(x) = \\int_{-\\infty}^{+\\infty} f(x)P(x_2)dx_2 = \\int_{-\\infty}^{+\\infty} (x_1 + x_2)*P(x_2)dx_2 = x_1\\int p(x_2) dx_2 + \\int x_2 p(x_2) dx_2 = x_1 + 2$$ \n",
    "\n",
    "Note that the second term is the expectation. Therefore, \n",
    "\n",
    "$$D_1 = var(f_1 (x)) = var(x_1 + 2) = var(x_1) + 0 = 9$$.\n",
    "\n",
    "In addition, $$f_2(x) = \\int_{-\\infty}^{+\\infty} f(x)P(x_1)dx_1 = \\int_{-\\infty}^{+\\infty} (x_1 + x_2)*P(x_1)dx_1 = x_2\\int p(x_1) dx_1 + \\int x_1 p(x_1) dx_1 = x_2 + 1 \\implies$$\n",
    "\n",
    "$$D_2 = var(f_2 (x)) = var(x_2 + 1) = var(x_2) + 0 = 4$$.\n",
    "\n",
    "$$f_0 = \\mu_1 + \\mu_2 = 3$$\n",
    "\n",
    "$$f_{12}(x) = f(x) - f_1 (x) - f_2 (x) - f_0 = x_1 + x_2 - x_1 - 2 - x_2 - 1 - 3 = -6\\  \\implies$$\n",
    "\n",
    "$$D_{12} = var(f_{12} (x)) = 0$$\n",
    "\n",
    "Finally,\n",
    "\n",
    "$$D = D_1 + D_2 + D_{12} = 9 + 4 = 13$$\n",
    "\n",
    "$$S_1 = \\frac{D_1}{D} = \\frac{9}{13}$$\n",
    "\n",
    "$$S_2 = \\frac{D_2}{D} = \\frac{4}{13}$$\n",
    "\n",
    "$$S_{12} = \\frac{D_{12}}{D} = \\frac{0}{13} = 0$$\n",
    "\n",
    "The model is additive since it is the addition of 2 random normal variables. Additionlly, there is no interaction since $D_{12}$ does not include $X_1$ neither $X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "(This problem is from Chapter 4 of the book: Andrea Saltelli, Debora Gatelli, Francesca Campolongo, Jessica Cariboni, Marco Ratto, Michaela Saisana, and Terry Andres, Global Sensitivity Analysis: The Primer, Wiley, 2008.)\n",
    "\n",
    "A model has eight input factors, but for computational cost's reasons we reduce the number of factors to five.\n",
    "The model is \n",
    " $Y = \\sum_{i=1}^{8} X_{i}$\n",
    " \n",
    " where $X_{i}$ is normally distributed as follows :\n",
    " \n",
    "$X_{i} \\sim N(0,1)$\n",
    "$X_{i} \\sim N(2,2)$\n",
    "$X_{i} \\sim N(1,3)$\n",
    "$X_{i} \\sim N(1,5)$\n",
    "$X_{i} \\sim N(3,1)$\n",
    "$X_{i} \\sim N(4,1)$\n",
    "$X_{i} \\sim N(1,2)$\n",
    "$X_{i} \\sim N(5,5)$\n",
    "\n",
    "(1) Calculate the first-order sensitivity indices (based on Sobol sensitivity analysis) and identify the three least important factors, in order to exclude them from the model.\n",
    "\n",
    "(2) Recalculate the first orders for the remaining five factors and find out which are the most influential: if we decide to fix them at a given value, in their range of variation, by what amount will the variance of the output be reduced? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) This model is almost similar to the one in problem 3. Hence, the factors that has the least effect on the model are those with the least variance. $$var(y) = \\sum_{i=1}^{n} var(x_i) = var(x_1) + var(x_2) + var(x_3) + var(x_4) + var(x_5) + var(x_6) + var(x_7) + var(x_8) = 1 + 4 + 9 + 25 + 1 + 1 + 4 + 25 = 70$$\n",
    "\n",
    "$$S_1 = \\frac{D_1}{D} = \\frac{1}{70}$$\n",
    "\n",
    "$$S_2 = \\frac{D_2}{D} = \\frac{4}{70}$$\n",
    "\n",
    "$$S_3 = \\frac{D_3}{D} = \\frac{9}{70}$$\n",
    "\n",
    "$$S_4 = \\frac{D_4}{D} = \\frac{25}{70}$$\n",
    "\n",
    "$$S_5 = \\frac{D_5}{D} = \\frac{1}{70}$$\n",
    "\n",
    "$$S_6 = \\frac{D_6}{D} = \\frac{1}{70}$$\n",
    "\n",
    "$$S_7 = \\frac{D_7}{D} = \\frac{4}{70}$$\n",
    "\n",
    "$$S_8 = \\frac{D_8}{D} = \\frac{25}{70}$$\n",
    "\n",
    "Where $D = Var(Y)$ and $D_i = V(E(Y|X_{\\sim i})$ which corresponds to the variance of the average $Y$ over all possible points of $X_i$, or the sensitivity/first order effect. Whenever this value is high, that is, close to 1 $\\implies$ $X_i$ has high effect on the output. When this value is close to 0 $\\implies$ there is no effect. Therefore, from above, variables, $X_1$, $X_5$ and $X_6$ are the least important variables. \n",
    "\n",
    "b) After removing the 3 least important factors, the new variance of the function is: \n",
    "\n",
    "$$var(y) = var(X_2) + var(X_3) + var(X_4) + var(X_7) + var(X_8) = 4 + 9 + 25 + 4 + 25 = 67$$\n",
    "\n",
    "then, the new sensitivity indecies: \n",
    "\n",
    "$$S_2 = \\frac{D_2}{D} = \\frac{4}{67}$$\n",
    "\n",
    "$$S_3 = \\frac{D_3}{D} = \\frac{9}{67}$$\n",
    "\n",
    "$$S_4 = \\frac{D_4}{D} = \\frac{25}{67}$$\n",
    "\n",
    "$$S_7 = \\frac{D_7}{D} = \\frac{4}{67}$$\n",
    "\n",
    "$$S_8 = \\frac{D_8}{D} = \\frac{25}{67}$$\n",
    "\n",
    "Hence, the most imfluential are $X_2$, $X_3$ and $X_8$. \n",
    "\n",
    "Finally, if we fixed these variables to some values in their range of variation, then the new variance will reduce to: $4 + 4 = 8$. Hence, the output variance will be reduces from 67 to 8 which is $1 - 8/67 \\approx 88$%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morris sensitivity analysis\n",
    "the Morris method for global sensitivity analysis is a so-called one-step-at-a-time method (OAT), meaning that in each run only one input parameter is given a new value. It facilitates a global sensitivity analysis by making a number r of local changes at different points x(1 → r) of the possible range of input values.[2]\n",
    "\n",
    "Steps:\n",
    "+ sample a set of values with defined range,which is used for inputs and then calculate the output.\n",
    "+ only change 1 variable can work out the output.\n",
    "+ repeat step two until all variables are changed.this process is usually repeated between 5 and 15 times.Such number is large enough compared to more demanding methods for sensitivity analysis.\n",
    "\n",
    "A sensitivity analysis method widely used to screen factors in models of large dimensionality is the design proposed by Morris.[3] The Morris method deals efficiently with models containing hundreds of input factors without relying on strict assumptions about the model, such as for instance additivity or monotonicity of the model input-output relationship. The Morris method is simple to understand and implement, and its results are easily interpreted. Furthermore, it is economic in the sense that it requires a number of model evaluations that is linear in the number of model factors. The method can be regarded as global as the final measure is obtained by averaging a number of local measures (the elementary effects), computed at different points of the input space.[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter                         Mu_Star         Mu    Mu_Star_Conf      Sigma\n",
      "x1                                  7.792      7.792           0.404      6.251\n",
      "x2                                  7.875      0.158           0.000      7.877\n",
      "x3                                  6.211      0.137           0.382      8.814\n"
     ]
    }
   ],
   "source": [
    "#morris\n",
    "import numpy as np\n",
    "import SALib as slb\n",
    "from SALib.analyze import morris\n",
    "from SALib.sample import morris\n",
    "from SALib.test_functions import Ishigami\n",
    "problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': ['x1', 'x2','x3'],\n",
    "    'bounds': [[-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359]]\n",
    "    }\n",
    "X = slb.sample.morris.sample(problem, 1000, num_levels=4, grid_jump=2)#sampling\n",
    "Y=Ishigami.evaluate(X)###the output \n",
    "Si=slb.analyze.morris.analyze(problem, X, Y, conf_level=0.95,print_to_console=True, num_levels=4, grid_jump=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to understand the result,we should first understand the Ishigami funtion,this funtion is defined as:\n",
    "$$Y=g(X_1,X_2,X_3)=sin(X_1)+asin^2(X_2)+bX_3^4sin(X_1)$$\n",
    "this model exhibits strong nonlinearity and nonmonotonicity. It also has a peculiar dependence on $X_3$\n",
    "in this model,$X_1,X_2,X_3$are independent random variables uniform in $[-\\pi,\\pi]$\n",
    "the value of a,b is a=7,b=0.1\n",
    "\n",
    "The mean and variance of the three inputs are represented by Mu and Sigma.Mu_star is the mean of the absolute values of the elementary effects,which is the best approximation of total sensitivity.However,it is not a direct interpretation as an attribution of variance,we should consider the Mu_star as a rank of the most sensitivity parameters,and in the above outcome,we can learn that X_1 is the most significant part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sensitivity analysis in metrology \n",
    "+ Derivative-based Global Sensitivity Measure\n",
    "\n",
    "The method is based on averaging local derivatives using Monte Carlo sampling methods. This method is much more accurate than the Morris method as the elementary effects are evaluated as local derivatives with only a small increments compared to the variable uncertainty ranges. It becomes especially efficient if automatic calculation of derivatives is used.[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter vi vi_std dgsm dgsm_conf\n",
      "x1 7.622378 16.198123 2.207554 1.001117\n",
      "x2 24.487757 17.338556 7.092019 1.026800\n",
      "x3 11.181258 24.062127 3.238259 1.531967\n"
     ]
    }
   ],
   "source": [
    "#Derivative-based Global Sensitivity Measure\n",
    "import numpy as np\n",
    "import SALib as slb\n",
    "from SALib.analyze import dgsm\n",
    "from SALib.sample import finite_diff\n",
    "from SALib.test_functions import Ishigami\n",
    "problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': ['x1', 'x2','x3'],\n",
    "    'bounds': [[-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359]]\n",
    "    }\n",
    "X= finite_diff.sample(problem, 1000,delta=0.001)## Generate samples\n",
    "Y=Ishigami.evaluate(X)###the output \n",
    "Si=dgsm.analyze(problem, X, Y, num_resamples=1000, conf_level=0.95, print_to_console=True)###analyze part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_i=E[(\\frac{\\partial Y}{\\partial x_i})^2]$,the index is motivated by the fact that a high value of the derivative of the model output with respect to any input variable leads to much variation of model output,so a higher value means a higher impact on the output of model,we can see that X2 is the highest among the three input,so X2 has a larger influence than th others,vi_std is the standard deviation of Vi,dgsm is the upper-bound to the total.[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ moment-independent sensitivity analysis\n",
    "\n",
    "The moment-independent SA focuses on finding those inputs that, if fixed at their distribution ranges, will lead to the greatest shift in the probability density function (PDF) of model output on average. It is also global, quantitative and model free, and additionally, it is moment-independent, thus attracts more and more attentions of practitioners recently. [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter delta delta_conf S1 S1_conf\n",
      "x1 0.177752 0.026942 0.275467 0.037069\n",
      "x2 0.225945 0.023294 0.277506 0.045769\n",
      "x3 0.129075 0.025875 0.009239 0.015652\n"
     ]
    }
   ],
   "source": [
    "#Delta Moment-Independent Measure\n",
    "import numpy as np\n",
    "import SALib as slb\n",
    "from SALib.sample import latin\n",
    "from SALib.analyze import delta\n",
    "from SALib.test_functions import Ishigami\n",
    "problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': ['x1', 'x2','x3'],\n",
    "    'bounds': [[-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359]]\n",
    "    }\n",
    "X= slb.sample.latin.sample(problem, 1000)## Generate samples\n",
    "Y=Ishigami.evaluate(X)###the output \n",
    "Si=slb.analyze.delta.analyze(problem,X, Y, conf_level=0.95, print_to_console=True)###analyze part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta is defined as $\\delta_i=\\frac{1}{2}E_{x_i}[s(Xi)]=\\frac{1}{2}\\int \\mid f_y(y)-f_{y\\mid x_i}(y)\\mid dy$,$\\delta_i$ represents the normalized expected shift in the distribution of Y provoked by Xi,we can see that X2 is the most influential parameter, S1 is the first-order indices we can also get the information that x2 is the most influential parameter[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Variance-based_sensitivity_analysis [Accessed 8 Mar, 2018]\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Morris_method [Accessed 8 Mar, 2018]\n",
    "\n",
    "[3] Morris, M.D. (1991). \"Factorial Sampling Plans for Preliminary Computational Experiments\" . Technometrics. 33: 161–174. doi:10.2307/1269043.\n",
    "\n",
    "[4] Campolongo, F.; Cariboni, J.; Saltelli, A. (2003). \"Sensitivity analysis: the Morris method versus the variance based measures\" . Submitted to Technometrics.\n",
    "\n",
    "[5]I.M. Sobol’a, S. Kucherenkob*.\"Derivative based global sensitivity measures \".Procedia Social and Behavioral Sciences 2 (2010) 7745–7746\n",
    "\n",
    "[6]Matthias De Lozzo, Amandine Marrel. Estimation of the derivative-based global sensitivity measures using a Gaussian process metamodel. 2015. <hal-01164215v2>\n",
    "\n",
    "[7]Pengfei Wei, Zhenzhou Lu n, Xiukai Yuan.\"Monte Carlo simulation for moment-independent sensitivity analysis\".Reliability Engineering and System Safety 110 (2013) 60–67\n",
    "\n",
    "[8]E. Borgonovo.A new uncertainty importance measure.Institute for Quantitative Methods, Bocconi University, 20135 Milano, Viale Isonzo 25, Italy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
