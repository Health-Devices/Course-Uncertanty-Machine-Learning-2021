{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "### Miodrag Bolic\n",
    "\n",
    "Due: April 25, 2021\n",
    "Total number of points is 10. \n",
    "\n",
    "Instructions:\n",
    "Upload your answers in a ipynb notebook to UOttawa Bright Space.\n",
    "\n",
    "Your individual submissions should use the following filenames: ELG_5218_YOURNAME_HW3.ipynb\n",
    "\n",
    "Your code should be in code cells as part of your notebook. Do not use any different format.\n",
    "\n",
    "*Do not just send your code. The homework solutions should be in a report style. Be sure to add comments to your code as well as markdown cells where you describe your approach and discuss your results. *\n",
    "\n",
    "Please submit your notebook in an executed status, so that we can see all the results you computed. However, we will still run your code and all cells should reproduce the output when executed.\n",
    "\n",
    "If you have multiple files (e.g. you've added code files or images) create a tarball for all files in a single file and name it: ELG_5218_YOURNAME_HW3.tar.gz or ELG_5218_YOURNAME_HW3.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 Time Series\n",
    "\n",
    "Below you find 200 data points from a time series. Your task is to model this data using a simple autoregressive model:\n",
    "\n",
    "$$\n",
    "X_t=\\alpha X_{t-1}+r_t\n",
    "$$\n",
    "where $r_t$ is normally distributed: $r_t \\sim\\mathcal{N}(0,\\sigma^2)$\n",
    "\n",
    "Here are your tasks:\n",
    "* Implement a maximum likelihood estimation(MLE) method to estimate $\\alpha$\n",
    "* Use any MCMC algorithm of your choice to infere $\\sigma^2$ and $\\alpha$\n",
    "* Forecast 3 time points ahead and provide posteriors for these three points\n",
    "\n",
    "#### Here is the data:\n",
    "([0.00000000e+00,  -2.41884381e-01,  -2.19815584e-01, -5.92654275e-03,   3.89498469e-02,  -7.06374756e-01,\n",
    "         6.09244734e-02,  -1.36663360e+00,  -1.63887006e+00, -1.31721354e+00,  -1.52424274e+00,  -1.23902096e+00,\n",
    "        -6.63029664e-01,  -5.97693396e-01,   1.26359198e-01, 6.55116724e-01,   2.98253859e-01,   7.19461104e-03,\n",
    "         4.04113046e-01,   7.29893878e-01,  -1.96731327e-03, 4.64230686e-01,  -6.79727540e-02,  -4.19137503e-01,\n",
    "        -4.03256074e-01,  -8.70131096e-01,  -8.25921060e-01, 4.69417714e-01,  -1.00177377e-01,   6.34076840e-02,\n",
    "        -1.05978178e-01,  -1.48916430e-01,  -5.43573786e-02, 4.61970441e-01,   8.18280558e-01,   2.87185308e-01,\n",
    "         1.26715777e+00,   6.41828280e-01,   8.77364512e-01, 3.67214345e-01,   6.26413729e-01,   2.87102039e-02,\n",
    "         4.28449069e-01,   7.31450267e-01,   6.19131504e-01, 6.33373751e-01,   3.17572088e-01,   3.51692600e-01,\n",
    "        -2.48995087e-02,  -2.53030153e-01,  -2.15503832e-01,-6.20238803e-01,  -5.28176551e-01,  -5.99112522e-01,\n",
    "         3.32834574e-01,   4.01426008e-01,  -6.37956657e-01,-4.52884655e-01,  -1.16203988e+00,  -8.82257305e-01,\n",
    "        -6.19309194e-01,  -4.78009274e-01,  -2.32960888e-01, 3.80860392e-01,  -5.52337194e-01,   6.43589528e-01,\n",
    "        -1.71939822e-01,  -2.22714248e-02,   1.69698173e-01, 2.26875861e-01,   6.27784254e-02,   9.59070565e-01,\n",
    "        -2.00412521e-01,   1.62853318e-02,  -1.64492388e-02,-5.92514844e-01,  -1.43386816e-01,  -9.50829381e-01,\n",
    "        -1.07109089e+00,  -5.47941202e-01,  -6.80501959e-01,-1.22073393e+00,  -5.09858830e-01,  -1.24176275e+00,\n",
    "        -6.30383282e-01,  -8.67182207e-01,  -9.67761290e-01,-1.79674059e-01,   6.09919157e-01,   9.95635469e-02,\n",
    "         2.28199545e-01,   4.21792446e-01,   1.69467875e-01,-2.59645693e-01,  -4.93207819e-01,  -5.01539736e-01,\n",
    "        -6.05873935e-01,  -6.11037064e-01,   1.01647103e-01,-1.44991643e-01,  -4.33238432e-02,  -6.41062073e-02,\n",
    "        -2.85514261e-01,  -3.95586179e-01,  -3.57194799e-01,-2.20170569e-01,  -3.39918202e-01,   1.70035788e-01,\n",
    "         7.41558107e-01,   8.33120964e-01,   3.73995334e-01, 4.83141624e-01,  -1.28861162e-01,  -4.22192149e-01,\n",
    "         5.78378631e-01,   1.43550174e-01,  -1.31226657e-01,-5.30885377e-01,  -8.76270272e-01,  -4.34154230e-01,\n",
    "        -5.02379201e-01,  -5.14585915e-01,  -4.64397849e-01, 1.65475660e-01,   1.07774616e-01,  -9.65587168e-01,\n",
    "        -8.60566293e-01,  -2.36405434e-01,  -4.12606502e-01,-5.58566210e-01,   1.20028400e+00,   9.96896069e-01,\n",
    "         4.41653164e-01,   1.59249052e-01,  -1.74168536e-01, 5.63049814e-01,   7.39366310e-01,   6.05361272e-01,\n",
    "         2.79509214e-01,  -6.26883609e-01,  -4.44982987e-01,-8.69359696e-01,  -9.27422224e-01,  -6.98179880e-01,\n",
    "        -1.35445853e+00,  -1.12329955e+00,  -4.94572306e-01,-3.40507834e-01,  -8.37907253e-01,  -9.97003760e-01,\n",
    "        -7.69088724e-01,  -4.77378429e-01,   4.52315984e-02, 8.76114754e-02,  -9.24336169e-02,   4.42178858e-01,\n",
    "         6.32190755e-01,   8.18383124e-01,   2.95975304e-01,-2.38797237e-02,   1.16395924e-01,   5.62311884e-02,\n",
    "        -3.15966789e-01,  -3.16808474e-01,  -1.16651970e+00,-1.01729199e+00,  -7.75285712e-01,  -4.17618624e-01,\n",
    "        -7.65156825e-01,  -5.10469718e-01,  -3.25334859e-01, 1.06358791e-01,   4.03467712e-01,   5.62837155e-01,\n",
    "         5.87793605e-01,   4.38191557e-01,  -4.17879322e-01,-3.19715350e-01,   3.41624528e-01,   4.93054502e-01,\n",
    "         3.69746949e-01,   9.62398763e-02,  -2.81485049e-01,-3.37776564e-01,  -1.30858217e-03,   9.83538405e-01,\n",
    "         6.94986165e-01,   2.41137851e-01,   2.79478944e-01, 5.13500357e-01,   4.17786188e-01,   7.46707509e-01,\n",
    "         1.25038407e+00,   8.17699739e-01,   8.78811468e-02,-1.20413802e-01,  -1.83227208e-01,   8.06195706e-02,\n",
    "         4.16043086e-01,   6.45585187e-02])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 GP Regression\n",
    "Generate (x, y) points from the following function $y = x \\cdot sin(x)$ where $x$ can take values 0, 2, 4, 6, 8, 10, 12 and 14.\n",
    "\n",
    "a)     Perform regression using Gaussian processes. Show the regression curve together with 95% confidence intervals. Try at least 2 kernels of your choice and explain results.\n",
    "\n",
    "b)     Find the mean and the variance of the prediction $y_*$ for the following values of $x_*$: 1, 14.5 and 18.\n",
    "\n",
    "c) Now generate points from $y = x \\cdot sin(x) + \\epsilon$ where $\\epsilon \\sim N(0,1)$. Perform regression using Gaussian processes. Show the regression curve together with 95% confidence intervals. Try at least 2 kernels of your choice and explain results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 Short questions about VAE\n",
    "1. Why is variational autoencoder considered to be generative models and autoencoder is not?\n",
    "2. Show (mathematical) relationship between the autoencoder and the propabilistic PCA? Draw autoencoder that is linear and represents PCA. \n",
    "3. What is amortized inference and how is it used in variational encoders? \n",
    "4. What is disentenglement? Discribe in several paragraph your understanding of a disentengled VAE that is not based on beta VAE - you would need to do your own research and one one paper that describes disentengled VAE - please include proper reference as well. \n",
    "5. Find engineering examples of disentanglement in variations encoders that is not related to images and videos - please include proper reference as well.\n",
    "6. Why would one want to have continous latent space vs. discrete latent space?\n",
    "7. Derive ELBO for the variational encoder with Binomial prior and variatioinal density? Follow https://davidstutz.de/bernoulli-variational-auto-encoder-in-torch/ but make sure that you explain all the steps. How would one train such a network?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 Short questions about PCA and FA\n",
    "1. What does it mean when one says: \"The propabilistic PCA and FA solutions are not unique.\" In what sense, the solutions are not unique? What solutions do they refer to?\n",
    "2. When is the probabilistic PCA equivalent to the original PCA? How are the coefficients W of the probabilistic PCA related to eigen vectors and eigen values?\n",
    "3. How would one know how to chose the dimension of of the latent variables in probabilistic PCA. How is that different from the original PCA?\n",
    "4. Derive Maximum likelihood method for the variance $\\sigma^2$ in probabilistic PCA.\n",
    "5. Present algorithmic steps for probabilistic PCA where the parameters are  estimated using maximum likelihood methods?\n",
    "6. Present algorithmic steps for the probabilistic PCA where the parameters are  estimated using variational inference.\n",
    "7. List major engineering application (again not related to image processing or computer vision) of probabilistic PCA and factor analysis - please include proper reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaPro_v1.5.3-1 1.5.3",
   "language": "julia",
   "name": "juliapro_v1.5.3-1-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
